{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Hacker News Post\n",
    "\n",
    "For this project we are analyzing the frequency of comments. The comments we are looking at derive from the community posts of \"Ask HN,\" \"Show HN,\", and also observing if any posts significantly receive more comments when posted at a certain time.\n",
    "\n",
    "I retrieved this dataset from the Hacker News Kaggle account. The dataset was updated approximately 2017.\n",
    "\n",
    "To note: In comparison with the DataQuest exercise data examples it is possible I received an updated or non-sampled version of the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First we will import and read the data\n",
    "\n",
    "And just to preview some entries we will display only the first five lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['id', 'title', 'url', 'num_points', 'num_comments', 'author', 'created_at'],\n",
       " ['12579008',\n",
       "  'You have two days to comment if you want stem cells to be classified as your own',\n",
       "  'http://www.regulations.gov/document?D=FDA-2015-D-3719-0018',\n",
       "  '1',\n",
       "  '0',\n",
       "  'altstar',\n",
       "  '9/26/2016 3:26'],\n",
       " ['12579005',\n",
       "  'SQLAR  the SQLite Archiver',\n",
       "  'https://www.sqlite.org/sqlar/doc/trunk/README.md',\n",
       "  '1',\n",
       "  '0',\n",
       "  'blacksqr',\n",
       "  '9/26/2016 3:24'],\n",
       " ['12578997',\n",
       "  'What if we just printed a flatscreen television on the side of our boxes?',\n",
       "  'https://medium.com/vanmoof/our-secrets-out-f21c1f03fdc8#.ietxmez43',\n",
       "  '1',\n",
       "  '0',\n",
       "  'pavel_lishin',\n",
       "  '9/26/2016 3:19'],\n",
       " ['12578989',\n",
       "  'algorithmic music',\n",
       "  'http://cacm.acm.org/magazines/2011/7/109891-algorithmic-composition/fulltext',\n",
       "  '1',\n",
       "  '0',\n",
       "  'poindontcare',\n",
       "  '9/26/2016 3:16']]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing and reading data as a list of lists\n",
    "#only displaying first five lines\n",
    "import csv\n",
    "\n",
    "hp = open(\"/home/main/Downloads/Exercises/HN_posts.csv\")\n",
    "hacker_posts = list(csv.reader(hp))\n",
    "hacker_posts[:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Above note: from the headers information we know what the data set contains such as number of comments which we will focus on later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'title', 'url', 'num_points', 'num_comments', 'author', 'created_at']\n",
      "[['12579008', 'You have two days to comment if you want stem cells to be classified as your own', 'http://www.regulations.gov/document?D=FDA-2015-D-3719-0018', '1', '0', 'altstar', '9/26/2016 3:26'], ['12579005', 'SQLAR  the SQLite Archiver', 'https://www.sqlite.org/sqlar/doc/trunk/README.md', '1', '0', 'blacksqr', '9/26/2016 3:24'], ['12578997', 'What if we just printed a flatscreen television on the side of our boxes?', 'https://medium.com/vanmoof/our-secrets-out-f21c1f03fdc8#.ietxmez43', '1', '0', 'pavel_lishin', '9/26/2016 3:19'], ['12578989', 'algorithmic music', 'http://cacm.acm.org/magazines/2011/7/109891-algorithmic-composition/fulltext', '1', '0', 'poindontcare', '9/26/2016 3:16'], ['12578979', 'How the Data Vault Enables the Next-Gen Data Warehouse and Data Lake', 'https://www.talend.com/blog/2016/05/12/talend-and-Â\\x93the-data-vaultÂ\\x94', '1', '0', 'markgainor1', '9/26/2016 3:14']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['12579008',\n",
       "  'You have two days to comment if you want stem cells to be classified as your own',\n",
       "  'http://www.regulations.gov/document?D=FDA-2015-D-3719-0018',\n",
       "  '1',\n",
       "  '0',\n",
       "  'altstar',\n",
       "  '9/26/2016 3:26'],\n",
       " ['12579005',\n",
       "  'SQLAR  the SQLite Archiver',\n",
       "  'https://www.sqlite.org/sqlar/doc/trunk/README.md',\n",
       "  '1',\n",
       "  '0',\n",
       "  'blacksqr',\n",
       "  '9/26/2016 3:24'],\n",
       " ['12578997',\n",
       "  'What if we just printed a flatscreen television on the side of our boxes?',\n",
       "  'https://medium.com/vanmoof/our-secrets-out-f21c1f03fdc8#.ietxmez43',\n",
       "  '1',\n",
       "  '0',\n",
       "  'pavel_lishin',\n",
       "  '9/26/2016 3:19'],\n",
       " ['12578989',\n",
       "  'algorithmic music',\n",
       "  'http://cacm.acm.org/magazines/2011/7/109891-algorithmic-composition/fulltext',\n",
       "  '1',\n",
       "  '0',\n",
       "  'poindontcare',\n",
       "  '9/26/2016 3:16'],\n",
       " ['12578979',\n",
       "  'How the Data Vault Enables the Next-Gen Data Warehouse and Data Lake',\n",
       "  'https://www.talend.com/blog/2016/05/12/talend-and-Â\\x93the-data-vaultÂ\\x94',\n",
       "  '1',\n",
       "  '0',\n",
       "  'markgainor1',\n",
       "  '9/26/2016 3:14']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing headers\n",
    "\n",
    "headers = hacker_posts[0]\n",
    "hacker_posts = hacker_posts[1:]\n",
    "\n",
    "\n",
    "print(headers)\n",
    "print(hacker_posts[:5])\n",
    "\n",
    "#Spreading out the data entries for readability\n",
    "sort_h = sorted(hacker_posts, reverse = True)\n",
    "sort_h[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating lists and sorting posts into three categories\n",
    "\n",
    "In the next section, we are creating three lists. As we are focused on determining comment frequency among Ask HN and Show HN we will have a list for each naturally. For any data entries not fitting the parameters we will have a third list, 'other post.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9139\n",
      "10158\n",
      "273822\n"
     ]
    }
   ],
   "source": [
    "#Creating three empty lists\n",
    "ask_posts = []\n",
    "show_posts = []\n",
    "other_posts = []\n",
    "\n",
    "#Looping through each row to sort and append\n",
    "for posts in hacker_posts:\n",
    "    title = posts[1]\n",
    "    \n",
    "    #appending based on the parameters of the starting text \n",
    "    if title.lower().startswith(\"ask hn\"): \n",
    "        ask_posts.append(posts)\n",
    "        \n",
    "    elif title.lower().startswith(\"show hn\"):\n",
    "        show_posts.append(posts)\n",
    "        \n",
    "    else:\n",
    "        other_posts.append(posts)\n",
    "        \n",
    "print(len(ask_posts))\n",
    "print(len(show_posts))\n",
    "print(len(other_posts))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining averages of the Ask and Show Hacker News posts\n",
    "\n",
    "We now have three distinct lists we can analyze individually. Let's calculate the averages of Ask HN and Show HN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "#Average comments for 'Ask HN'\n",
    "# num_comments is the fifth column in the dataset therefore index is 4\n",
    "\n",
    "total_ask_comments = 0\n",
    "\n",
    "for posts in ask_posts:\n",
    "    total_ask_comments += int(posts[4])\n",
    "    \n",
    "avg_ask_comments = total_ask_comments / len(ask_posts)\n",
    "\n",
    "\n",
    "\n",
    "print(round(avg_ask_comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "#Average comments for 'Show HN'\n",
    "\n",
    "total_show_comments = 0\n",
    "\n",
    "for posts in show_posts:\n",
    "    total_show_comments += int(posts[4])\n",
    "    \n",
    "avg_show_comments = total_show_comments / len(show_posts)\n",
    "\n",
    "\n",
    "print(round(avg_show_comments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is clear, from the data, Ask HN receives more activity than Show HN. We will delve more into the analysis on the Ask HN posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining the number of Ask Posts and Comments created by Hour\n",
    "\n",
    "Here we are creating two dictionaries. \"Counts by hour\" will contain the number of Ask Posts created during each hour of the day. \"Comment by hour\" will have the corresponding number of comments the Ask Posts were created at each hour received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'02': 2996,\n",
       " '01': 2089,\n",
       " '22': 3372,\n",
       " '21': 4500,\n",
       " '19': 3954,\n",
       " '17': 5547,\n",
       " '15': 18525,\n",
       " '14': 4972,\n",
       " '13': 7245,\n",
       " '11': 2797,\n",
       " '10': 3013,\n",
       " '09': 1477,\n",
       " '07': 1585,\n",
       " '03': 2154,\n",
       " '23': 2297,\n",
       " '20': 4462,\n",
       " '16': 4466,\n",
       " '08': 2362,\n",
       " '00': 2277,\n",
       " '18': 4877,\n",
       " '12': 4234,\n",
       " '04': 2360,\n",
       " '06': 1587,\n",
       " '05': 1838}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Determining amount of Ask HN posts created during each hour of the day\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "result_list = []\n",
    "\n",
    "#created_at is on column 7 so index is 6\n",
    "for post in ask_posts:\n",
    "    result_list.append([post[6], int(post[4])])\n",
    "\n",
    "#creating two empty dictionaries\n",
    "counts_by_hour = {}\n",
    "comments_by_hour = {}\n",
    "\n",
    "\n",
    "date_format = \"%m/%d/%Y %H:%M\"\n",
    "\n",
    "for each_row in result_list:\n",
    "    date = each_row[0]\n",
    "    comment = each_row[1]\n",
    "    \n",
    "    time = dt.datetime.strptime(date, date_format).strftime(\"%H\")\n",
    "    \n",
    "    if time in counts_by_hour:\n",
    "        comments_by_hour[time] += comment\n",
    "        counts_by_hour[time] += 1\n",
    "    else:\n",
    "        comments_by_hour[time] = comment\n",
    "        counts_by_hour[time] = 1\n",
    "\n",
    "comments_by_hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining the average of Ask HN posts by hour\n",
    "\n",
    "With the above information we can now calculate the average amount of comments Ask Posts created at each hour of the day received.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['02', 11.137546468401487],\n",
       " ['01', 7.407801418439717],\n",
       " ['22', 8.804177545691905],\n",
       " ['21', 8.687258687258687],\n",
       " ['19', 7.163043478260869],\n",
       " ['17', 9.449744463373083],\n",
       " ['15', 28.676470588235293],\n",
       " ['14', 9.692007797270955],\n",
       " ['13', 16.31756756756757],\n",
       " ['11', 8.96474358974359],\n",
       " ['10', 10.684397163120567],\n",
       " ['09', 6.653153153153153],\n",
       " ['07', 7.013274336283186],\n",
       " ['03', 7.948339483394834],\n",
       " ['23', 6.696793002915452],\n",
       " ['20', 8.749019607843136],\n",
       " ['16', 7.713298791018998],\n",
       " ['08', 9.190661478599221],\n",
       " ['00', 7.5647840531561465],\n",
       " ['18', 7.94299674267101],\n",
       " ['12', 12.380116959064328],\n",
       " ['04', 9.7119341563786],\n",
       " ['06', 6.782051282051282],\n",
       " ['05', 8.794258373205741]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Determining average amount of comments of Ask HN posts created at each hour of the day received\n",
    "\n",
    "avg_by_hour = []\n",
    "\n",
    "for hour in comments_by_hour:\n",
    "    avg_by_hour.append([hour, comments_by_hour[hour] / counts_by_hour[hour]])\n",
    "\n",
    "avg_by_hour\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting and Printing the highest average comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11.137546468401487, '02'], [7.407801418439717, '01'], [8.804177545691905, '22'], [8.687258687258687, '21'], [7.163043478260869, '19'], [9.449744463373083, '17'], [28.676470588235293, '15'], [9.692007797270955, '14'], [16.31756756756757, '13'], [8.96474358974359, '11'], [10.684397163120567, '10'], [6.653153153153153, '09'], [7.013274336283186, '07'], [7.948339483394834, '03'], [6.696793002915452, '23'], [8.749019607843136, '20'], [7.713298791018998, '16'], [9.190661478599221, '08'], [7.5647840531561465, '00'], [7.94299674267101, '18'], [12.380116959064328, '12'], [9.7119341563786, '04'], [6.782051282051282, '06'], [8.794258373205741, '05']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[28.676470588235293, '15'],\n",
       " [16.31756756756757, '13'],\n",
       " [12.380116959064328, '12'],\n",
       " [11.137546468401487, '02'],\n",
       " [10.684397163120567, '10'],\n",
       " [9.7119341563786, '04'],\n",
       " [9.692007797270955, '14'],\n",
       " [9.449744463373083, '17'],\n",
       " [9.190661478599221, '08'],\n",
       " [8.96474358974359, '11'],\n",
       " [8.804177545691905, '22'],\n",
       " [8.794258373205741, '05'],\n",
       " [8.749019607843136, '20'],\n",
       " [8.687258687258687, '21'],\n",
       " [7.948339483394834, '03'],\n",
       " [7.94299674267101, '18'],\n",
       " [7.713298791018998, '16'],\n",
       " [7.5647840531561465, '00'],\n",
       " [7.407801418439717, '01'],\n",
       " [7.163043478260869, '19'],\n",
       " [7.013274336283186, '07'],\n",
       " [6.782051282051282, '06'],\n",
       " [6.696793002915452, '23'],\n",
       " [6.653153153153153, '09']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create new list\n",
    "swap_avg_by_hour = []\n",
    "\n",
    "for row in avg_by_hour:\n",
    "    swap_avg_by_hour.append([row[1], row[0]])\n",
    "    \n",
    "print(swap_avg_by_hour)\n",
    "\n",
    "#lets sort for readability\n",
    "sort = sorted(swap_avg_by_hour, reverse = True)\n",
    "sort\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Hours for 'Ask HN' Comments\n",
      "15:00: 28.68 average comments per post\n",
      "13:00: 16.32 average comments per post\n",
      "12:00: 12.38 average comments per post\n",
      "02:00: 11.14 average comments per post\n",
      "10:00: 10.68 average comments per post\n"
     ]
    }
   ],
   "source": [
    "#Sorting and printing the values of top 5 hours w/ highest avg comments.\n",
    "\n",
    "print(\"Top 5 Hours for 'Ask HN' Comments\")\n",
    "for avg, hr in sort[:5]:\n",
    "    print(\"{}: {:.2f} average comments per post\".\n",
    "          format(dt.datetime.strptime(hr, \"%H\").strftime(\"%H:%M\"),avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "The highest amount of comments is at the 15:00 hour. From 12:00 to 15:00 the average increases greatly over 5 hours. This time period range are typical lunch hour times; considering any day of the week we can deduce food makes HN users more engaged.\n",
    "\n",
    "We could inlude average commenting numbers in the Show HN to support this theory to see if they align closely with the Ask HN results.\n",
    "\n",
    "Another route to take is we could include posts that didn't receive comments at all. We would be delving further into the \"Other_posts\" list as created in the analysis. From that information, we can conduct further analysis to determine what times have the least amount of engagement. Considering all three lists to determine to \"Top 5 hours of Overall least Comments/Engagement\" we can possibly create a marketing campaign or utilize simple engagement tactics to increase commenting and engagement traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
